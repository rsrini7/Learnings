### Google Gemini 3 Flash — Dec 16, 2025
- Lightweight but **frontier‑level** Gemini model optimized for speed, low latency, and lower cost, now rolling out broadly (including as default in many Gemini experiences).[2][3]
- Targets real‑time and agentic workloads (coding, reasoning, multimodal) with better performance than Gemini 2.5 Pro at a fraction of the cost.[2][4]

### Google FunctionGemma — Dec 17, 2025
- 270M‑parameter Gemma 3 variant tuned specifically for structured function/tool calling rather than general chat, ideal as a base for on‑device agents.[5][6]
- Uses constrained formatting so outputs are easy to parse into API calls, making it simpler to orchestrate workflows, databases, and tools programmatically.[7][8]

### Google T5 Gemma 2 — Dec 2025
- New encoder–decoder LLM line combining Gemma 2 with T5‑style sequence‑to‑sequence strengths, supporting multimodal inputs and long context (up to 128K tokens) for enterprise tasks.[9]
- Designed to improve math, coding, and multilingual reasoning with high accuracy and relatively low latency for production and research workloads.[9]

### Gemini CLI Conductor — Dec 2025
- Gemini CLI extension “Conductor” stores specs, plans, and guardrails as Markdown inside the repo, enforcing “plan before code” for agentic development workflows.[10]
- Helps Gemini act more like a **teammate** over a stable architecture description instead of a stateless autocomplete, improving reliability on large codebases.[10]

### Google Opal Vibe‑Coding in Gemini — Dec 2025
- Opal tooling now in Gemini lets users create “Gems” (custom mini‑apps/agents) via natural language, then connect and tweak them visually without coding.[9]
- Lowers the barrier to building workflow‑specific AI agents, speeding prototyping for both developers and non‑technical users.[9]

### ChatGPT Images — Dec 2025
- New ChatGPT feature to generate and edit images directly in the chat interface with faster, more precise visual editing than earlier DALL·E‑style flows.[11]
- Keeps text and image workflows in a single place so teams can iterate on copy and visuals together without switching tools.[11]

### GPT‑5.2‑Codex — Dec 18–21, 2025
- OpenAI’s most advanced agentic coding model focused on real‑world software engineering and defensive cybersecurity, built on the GPT‑5.2 family.[12][13]
- Uses long‑horizon context handling and “context compaction” to support large refactors, migrations, and automated vulnerability detection with safeguards like sandboxing.[14][12]

### ChatGPT App Marketplace — Dec 2025
- OpenAI opens an in‑ChatGPT marketplace where developers can submit apps that plug directly into ChatGPT, turning it into an app ecosystem rather than a single chatbot.[15][11]
- Gives developers instant distribution to millions of users and enables monetizable, integrated AI tools without separate standalone products.[15]

### Anthropic Agent Skills Open Standard — Dec 2025
- Anthropic’s modular “Agent Skills” format is now an open standard so skills can be built, shared, and reused across agents and platforms.[11]
- Makes it easier to compose complex agents from reusable capabilities, improving interoperability and reducing custom one‑off code.[11]

### Mistral OCR 3 — Dec 2025
- New OCR model from Mistral with large gains on forms, scans, tables, and handwriting, reporting ~74% win rate against prior systems on document benchmarks.[11]
- Reconstructs tables with HTML‑like structure and supports structured outputs, enabling high‑fidelity, cost‑effective document digitization pipelines.[11]

### Brave AI Browsing (Leo Agent) — Dec 2025
- Brave Nightly adds an early AI browsing mode where Leo can perform agentic tasks like research, shopping flows, and summarizing sites in an isolated profile.[11]
- Designed to automate common browsing tasks while keeping strict privacy controls by separating AI activity from main browsing data.[11]

### xAI Grok Voice Agent API — Dec 2025
- Grok Voice Agent API lets developers build low‑latency, expressive voice agents with support for 100+ languages and near–real‑time responses.[11]
- Agents can search the web and X, call custom tools, and answer context‑aware queries with sub‑second “first audio” latency.[11]

### Meta SAM Audio — Dec 2025
- Meta’s SAM Audio is a unified model for separating individual sounds from mixtures via text, visual, or time prompts.[11]
- Enables precise isolation of voices, instruments, or noise, simplifying editing for music, podcasts, film, accessibility, and research.[11]

### Claude in Chrome (Anthropic) — Dec 2025
- Experimental Chrome integration brings Claude directly into the browser for paid users, giving inline AI help while reading or working on web content.[11]
- Reduces context‑switching by letting users draft, summarize, and analyze information without leaving the tab they are on.[11]

### Firecrawl Agent Mode — Dec 2025
- Firecrawl’s `/agent` mode can autonomously crawl and interact with websites (clicking, navigating, handling dynamic content) without predefined URLs or scripts.[11]
- Useful for scalable lead generation, competitive research, and dataset building, replacing brittle, hand‑written scraping workflows.[11]

[2](https://blog.google/products/gemini/gemini-3-flash/)
[3](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-flash)
[4](https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-flash-for-enterprises)
[5](https://blog.google/technology/developers/functiongemma/)
[6](https://huggingface.co/google/functiongemma-270m-it)
[7](https://ai.google.dev/gemma/docs/capabilities/function-calling)
[8](https://www.philschmid.de/gemma-function-calling)
[9](https://ai.google.dev/gemini-api/docs/models)
[10](https://developers.googleblog.com/gemini-3-flash-is-now-available-in-gemini-cli/)
[11](https://platform.openai.com/docs/models/gpt-5.2)
[12](https://www.gend.co/blog/gpt-5-2-codex)
[13](https://openai.com/index/gpt-5-2-codex-system-card/)
[14](https://cyberpress.org/openai-gpt-5-2-codex-vulnerability-detection/)
[15](https://openai.com/index/introducing-gpt-5-2-codex/)
[17](https://deepmind.google/models/gemini/flash/)
[18](https://aistudio.google.com/models/gemini-3)
[19](https://www.reddit.com/r/singularity/comments/1q1gmck/gemini_3_flash_tops_the_new_misguided_attention/)
[20](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-3-flash-preview)
[21](https://www.youtube.com/watch?v=-Tgc_9uYJLI)
[22](https://github.blog/changelog/2025-12-17-gemini-3-flash-is-now-in-public-preview-for-github-copilot/)